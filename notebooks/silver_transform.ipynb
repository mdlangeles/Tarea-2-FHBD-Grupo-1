{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb9bbf29-b951-4c32-a6cc-b308f981ef0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bb9fd9bb-ba8b-41d5-8a69-b4c658c875c8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.96.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 393ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.96.1 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-bb9fd9bb-ba8b-41d5-8a69-b4c658c875c8\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/16ms)\n",
      "25/10/13 19:30:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "def create_silver_spark_session():\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"SilverLayer\")\n",
    "        .config('spark.jars.packages', \n",
    "                'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,'\n",
    "                'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.96.1,'\n",
    "                'org.apache.hadoop:hadoop-aws:3.3.4')\n",
    "        .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v1\")\n",
    "        .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "        .config(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\")\n",
    "        .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://lakehouse/\")\n",
    "        .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\")\n",
    "        .config(\"spark.sql.catalog.nessie.s3.access-key-id\", \"minioadmin\")\n",
    "        .config(\"spark.sql.catalog.nessie.s3.secret-access-key\", \"minioadmin\")\n",
    "        .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "        .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\")\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        # Configuraciones para mejorar estabilidad\n",
    "        .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\")\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    return spark\n",
    "    \n",
    "spark = create_silver_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5237840-3569-4431-bdb3-f3d06a8655f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bronze_data(table_name, limit=None):\n",
    "    \"\"\"\n",
    "    Lee datos de la capa Bronze desde MinIO\n",
    "    \"\"\"\n",
    "    bronze_path = f\"s3a://lakehouse/comments_2021/comments_2021/{table_name}\"\n",
    "    \n",
    "    df = spark.read.parquet(bronze_path)\n",
    "    \n",
    "    if limit:\n",
    "        df = df.limit(limit)\n",
    "        \n",
    "    print(f\"Leídos {df.count()} registros de {table_name}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd56a9a0-3c24-4737-965d-9a18016b6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_comments_to_silver():\n",
    "    \"\"\"\n",
    "    Transforma datos de comments - los textos están en formato binary/UTF-8, no Base64\n",
    "    \"\"\"\n",
    "    # Leer datos de bronze\n",
    "    comments_df = read_bronze_data(\"1760381569.7765708.45804a8704.parquet\", limit=100000)\n",
    "    \n",
    "    # Agregar columna de fecha de cargue\n",
    "    current_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Función UDF para convertir binary a string UTF-8\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.types import StringType\n",
    "    \n",
    "    def binary_to_utf8(binary_data):\n",
    "        try:\n",
    "            if binary_data is None:\n",
    "                return None\n",
    "            return binary_data.decode('utf-8')\n",
    "        except Exception as e:\n",
    "            return f\"[DECODE_ERROR: {str(e)}]\"\n",
    "    \n",
    "    binary_to_utf8_udf = udf(binary_to_utf8, StringType())\n",
    "    \n",
    "    silver_comments = (\n",
    "        comments_df\n",
    "        .withColumn(\"load_date\", lit(current_timestamp).cast(\"timestamp\"))\n",
    "        .withColumn(\"comment_year\", year(to_timestamp(col(\"creation_date\"))))\n",
    "        .withColumn(\"comment_month\", month(to_timestamp(col(\"creation_date\"))))\n",
    "        .withColumn(\"comment_day\", dayofmonth(to_timestamp(col(\"creation_date\"))))\n",
    "        .withColumnRenamed(\"id\", \"comment_id\")\n",
    "        .withColumnRenamed(\"text\", \"comment_text_binary\")\n",
    "        # Convertir binary a string UTF-8\n",
    "        .withColumn(\"comment_text\", binary_to_utf8_udf(col(\"comment_text_binary\")))\n",
    "        .withColumn(\"user_display_name_decoded\", \n",
    "                   binary_to_utf8_udf(col(\"user_display_name\")))\n",
    "        .withColumn(\"text_length\", \n",
    "                   when(col(\"comment_text\").isNotNull(), \n",
    "                        length(col(\"comment_text\")))\n",
    "                   .otherwise(0))\n",
    "        .withColumn(\"has_user_display_name\", \n",
    "                   when(col(\"user_display_name_decoded\").isNull() | \n",
    "                        (col(\"user_display_name_decoded\") == \"\"), \n",
    "                        False).otherwise(True))\n",
    "        .withColumn(\"score_category\",\n",
    "                   when(col(\"score\") >= 5, \"high\")\n",
    "                   .when(col(\"score\") >= 1, \"medium\")\n",
    "                   .otherwise(\"low\"))\n",
    "        .withColumn(\"is_text_decoded\", \n",
    "                   ~col(\"comment_text\").contains(\"[DECODE_ERROR]\"))\n",
    "        .select(\n",
    "            \"comment_id\",\n",
    "            \"post_id\",\n",
    "            \"score\",\n",
    "            \"score_category\",\n",
    "            \"comment_text\",  # Texto decodificado\n",
    "            \"text_length\",\n",
    "            \"is_text_decoded\",\n",
    "            \"creation_date\",\n",
    "            \"comment_year\",\n",
    "            \"comment_month\", \n",
    "            \"comment_day\",\n",
    "            \"user_id\",\n",
    "            \"user_display_name_decoded\",  # Display name decodificado\n",
    "            \"has_user_display_name\",\n",
    "            \"load_date\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return silver_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "467e5866-ad9a-4717-8d94-414bf3b11b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFIGURANDO NAMESPACES EN NESSIE ===\n",
      "Namespaces existentes:\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   silver|\n",
      "+---------+\n",
      "\n",
      "Creando namespace 'silver'...\n",
      "✅ Namespace 'silver' creado exitosamente\n",
      "Namespaces después de la creación:\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   silver|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def setup_nessie_namespaces():\n",
    "    \"\"\"\n",
    "    Crear los namespaces necesarios en Nessie\n",
    "    \"\"\"\n",
    "    print(\"=== CONFIGURANDO NAMESPACES EN NESSIE ===\")\n",
    "    \n",
    "    try:\n",
    "        # Verificar namespaces existentes\n",
    "        print(\"Namespaces existentes:\")\n",
    "        spark.sql(\"SHOW NAMESPACES IN nessie\").show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error mostrando namespaces: {e}\")\n",
    "    \n",
    "    # Crear namespace silver si no existe\n",
    "    try:\n",
    "        print(\"Creando namespace 'silver'...\")\n",
    "        spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.silver\")\n",
    "        print(\"✅ Namespace 'silver' creado exitosamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creando namespace silver: {e}\")\n",
    "        # Intentar con comando alternativo\n",
    "        try:\n",
    "            spark.sql(\"CREATE SCHEMA IF NOT EXISTS nessie.silver\")\n",
    "            print(\"✅ Schema 'silver' creado exitosamente\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Error creando schema: {e2}\")\n",
    "    \n",
    "    # Verificar que se creó\n",
    "    try:\n",
    "        print(\"Namespaces después de la creación:\")\n",
    "        spark.sql(\"SHOW NAMESPACES IN nessie\").show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error verificando namespaces: {e}\")\n",
    "\n",
    "# Ejecutar la configuración primero\n",
    "setup_nessie_namespaces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94c3a5f3-3520-40a5-9447-ed3563b03682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_into_silver_table(silver_df, table_name, key_columns):\n",
    "    \"\"\"\n",
    "    Realiza MERGE en tabla Iceberg con mejor manejo de errores\n",
    "    \"\"\"\n",
    "    silver_table_path = f\"nessie.silver.{table_name}\"\n",
    "    \n",
    "    print(f\"Intentando MERGE en: {silver_table_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Verificar si la tabla existe\n",
    "        spark.sql(f\"DESCRIBE {silver_table_path}\").show()\n",
    "        table_exists = True\n",
    "        print(f\"Tabla {silver_table_path} existe\")\n",
    "    except Exception as e:\n",
    "        table_exists = False\n",
    "        print(f\"Tabla {silver_table_path} no existe, creándola: {e}\")\n",
    "    \n",
    "    if not table_exists:\n",
    "        print(f\"Creando nueva tabla Iceberg: {silver_table_path}\")\n",
    "        try:\n",
    "            # Crear la tabla si no existe\n",
    "            (silver_df\n",
    "             .writeTo(silver_table_path)\n",
    "             .using(\"iceberg\")\n",
    "             .createOrReplace())\n",
    "            print(\"Tabla creada exitosamente\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creando tabla: {e}\")\n",
    "            # Intentar con CREATE TABLE SQL\n",
    "            try:\n",
    "                # Crear tabla manualmente\n",
    "                create_table_sql = f\"\"\"\n",
    "                CREATE TABLE {silver_table_path} (\n",
    "                    comment_id LONG,\n",
    "                    post_id LONG,\n",
    "                    score LONG,\n",
    "                    score_category STRING,\n",
    "                    comment_text STRING,\n",
    "                    text_length INT,\n",
    "                    is_text_decoded BOOLEAN,\n",
    "                    creation_date TIMESTAMP,\n",
    "                    comment_year INT,\n",
    "                    comment_month INT,\n",
    "                    comment_day INT,\n",
    "                    user_id LONG,\n",
    "                    user_display_name_decoded STRING,\n",
    "                    has_user_display_name BOOLEAN,\n",
    "                    load_date TIMESTAMP\n",
    "                ) USING iceberg\n",
    "                \"\"\"\n",
    "                spark.sql(create_table_sql)\n",
    "                print(\"Tabla creada con SQL exitosamente\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Error creando tabla con SQL: {e2}\")\n",
    "                return\n",
    "    else:\n",
    "        print(f\"Realizando MERGE en tabla existente: {silver_table_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Crear temp view para el MERGE\n",
    "            silver_df.createOrReplaceTempView(\"updates\")\n",
    "            \n",
    "            # Construir condición ON dinámica\n",
    "            on_condition = \" AND \".join([f\"target.{col} = updates.{col}\" for col in key_columns])\n",
    "            \n",
    "            # Construir columnas para UPDATE (excluyendo keys)\n",
    "            update_columns = [col for col in silver_df.columns if col not in key_columns]\n",
    "            update_set = \", \".join([f\"{col} = updates.{col}\" for col in update_columns])\n",
    "            \n",
    "            # Construir columnas para INSERT\n",
    "            insert_columns = \", \".join(silver_df.columns)\n",
    "            insert_values = \", \".join([f\"updates.{col}\" for col in silver_df.columns])\n",
    "            \n",
    "            # Ejecutar MERGE\n",
    "            merge_sql = f\"\"\"\n",
    "            MERGE INTO {silver_table_path} AS target\n",
    "            USING updates\n",
    "            ON {on_condition}\n",
    "            WHEN MATCHED THEN\n",
    "                UPDATE SET {update_set}\n",
    "            WHEN NOT MATCHED THEN\n",
    "                INSERT ({insert_columns})\n",
    "                VALUES ({insert_values})\n",
    "            \"\"\"\n",
    "            \n",
    "            print(\"Ejecutando MERGE...\")\n",
    "            spark.sql(merge_sql)\n",
    "            print(\"MERGE completado exitosamente\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en MERGE: {e}\")\n",
    "            print(\"Intentando INSERT OVERWRITE como fallback...\")\n",
    "            try:\n",
    "                # Fallback: usar overwrite\n",
    "                (silver_df\n",
    "                 .writeTo(silver_table_path)\n",
    "                 .using(\"iceberg\")\n",
    "                 .overwrite())\n",
    "                print(\"OVERWRITE completado exitosamente\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Error en OVERWRITE: {e2}\")\n",
    "    \n",
    "    # Mostrar estadísticas\n",
    "    try:\n",
    "        final_count = spark.sql(f\"SELECT COUNT(*) as total FROM {silver_table_path}\").collect()[0]['total']\n",
    "        print(f\"Total registros en {table_name}: {final_count}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error contando registros: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d44fd382-aaad-47a7-be84-53184910c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_layer():\n",
    "    \"\"\"\n",
    "    Proceso completo para transformar Bronze → Silver\n",
    "    \"\"\"\n",
    "    print(\"=== INICIANDO PROCESO SILVER LAYER ===\")\n",
    "    \n",
    "    # 1. Configurar namespace primero\n",
    "    setup_nessie_namespaces()\n",
    "    \n",
    "    # 2. Transformar Comments\n",
    "    print(\"2. Transformando Comments...\")\n",
    "    silver_comments = transform_comments_to_silver()\n",
    "    \n",
    "    # Forzar evaluación\n",
    "    silver_comments = silver_comments.cache()\n",
    "    record_count = silver_comments.count()\n",
    "    print(f\"Transformados {record_count} registros\")\n",
    "    \n",
    "    print(\"Muestra de datos transformados:\")\n",
    "    silver_comments.select(\n",
    "        \"comment_id\", \"post_id\", \"score\", \"score_category\",\n",
    "        \"text_length\", \"creation_date\", \"user_id\", \"is_text_decoded\"\n",
    "    ).show(10, truncate=False)\n",
    "    \n",
    "    # 3. Estadísticas de decodificación\n",
    "    print(\"3. Estadísticas de decodificación:\")\n",
    "    silver_comments.groupBy(\"is_text_decoded\").count().show()\n",
    "    \n",
    "    # 4. Hacer MERGE a tabla Iceberg\n",
    "    valid_comments = silver_comments.filter(col(\"is_text_decoded\") == True)\n",
    "    \n",
    "    if valid_comments.count() > 0:\n",
    "        print(f\"4. Realizando MERGE con {valid_comments.count()} comentarios válidos...\")\n",
    "        \n",
    "        # Usar método más simple para la primera creación\n",
    "        try:\n",
    "            # Primero crear la tabla si no existe\n",
    "            (valid_comments.limit(1)  # Solo schema\n",
    "             .writeTo(\"nessie.silver.comments\")\n",
    "             .using(\"iceberg\")\n",
    "             .createOrReplace())\n",
    "            print(\"✅ Tabla 'comments' creada exitosamente\")\n",
    "            \n",
    "            # Luego hacer append de los datos\n",
    "            valid_comments.writeTo(\"nessie.silver.comments\").append()\n",
    "            print(\"✅ Datos insertados exitosamente\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error en proceso principal: {e}\")\n",
    "            print(\"Intentando método alternativo...\")\n",
    "            \n",
    "            # Método alternativo: crear con SQL\n",
    "            try:\n",
    "                create_sql = \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS nessie.silver.comments (\n",
    "                    comment_id LONG,\n",
    "                    post_id LONG,\n",
    "                    score LONG,\n",
    "                    score_category STRING,\n",
    "                    comment_text STRING,\n",
    "                    text_length INT,\n",
    "                    is_text_decoded BOOLEAN,\n",
    "                    creation_date TIMESTAMP,\n",
    "                    comment_year INT,\n",
    "                    comment_month INT,\n",
    "                    comment_day INT,\n",
    "                    user_id LONG,\n",
    "                    user_display_name_decoded STRING,\n",
    "                    has_user_display_name BOOLEAN,\n",
    "                    load_date TIMESTAMP\n",
    "                ) USING iceberg\n",
    "                \"\"\"\n",
    "                spark.sql(create_sql)\n",
    "                print(\"✅ Tabla creada con SQL\")\n",
    "                \n",
    "                # Insertar datos\n",
    "                valid_comments.writeTo(\"nessie.silver.comments\").append()\n",
    "                print(\"✅ Datos insertados\")\n",
    "                \n",
    "            except Exception as e2:\n",
    "                print(f\"❌ Error método alternativo: {e2}\")\n",
    "    else:\n",
    "        print(\"4. No hay comentarios válidos para procesar\")\n",
    "    \n",
    "    # 5. Verificar datos en Silver\n",
    "    print(\"5. Verificando datos en Silver...\")\n",
    "    try:\n",
    "        spark.sql(\"SHOW TABLES IN nessie.silver\").show()\n",
    "        spark.sql(\"SELECT COUNT(*) as total FROM nessie.silver.comments\").show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error verificando datos: {e}\")\n",
    "    \n",
    "    print(\"=== PROCESO SILVER COMPLETADO ===\")\n",
    "    \n",
    "    return silver_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e2d82d-a128-4849-861d-30307f7b6fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO PROCESO SILVER LAYER ===\n",
      "=== CONFIGURANDO NAMESPACES EN NESSIE ===\n",
      "Namespaces existentes:\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   silver|\n",
      "+---------+\n",
      "\n",
      "Creando namespace 'silver'...\n",
      "✅ Namespace 'silver' creado exitosamente\n",
      "Namespaces después de la creación:\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   silver|\n",
      "+---------+\n",
      "\n",
      "2. Transformando Comments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/13 19:30:18 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leídos 100000 registros de 1760381569.7765708.45804a8704.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformados 100000 registros\n",
      "Muestra de datos transformados:\n",
      "+----------+-------+-----+--------------+-----------+-----------------------+--------+---------------+\n",
      "|comment_id|post_id|score|score_category|text_length|creation_date          |user_id |is_text_decoded|\n",
      "+----------+-------+-----+--------------+-----------+-----------------------+--------+---------------+\n",
      "|116230672 |5249797|0    |low           |160        |2021-01-15 15:32:10.917|1999993 |true           |\n",
      "|120551781 |5250531|0    |low           |105        |2021-07-01 12:06:14.767|144408  |true           |\n",
      "|123758678 |5255237|0    |low           |504        |2021-11-17 19:04:42.303|1028230 |true           |\n",
      "|117245192 |5256426|1    |medium        |355        |2021-02-22 15:16:04.207|1250772 |true           |\n",
      "|121632250 |5256470|0    |low           |134        |2021-08-17 21:00:45.4  |13738662|true           |\n",
      "|123328265 |5259967|0    |low           |116        |2021-10-29 14:55:05.95 |40899   |true           |\n",
      "|120487510 |5262227|0    |low           |32         |2021-06-29 04:37:49.583|8323067 |true           |\n",
      "|118891535 |5264449|0    |low           |199        |2021-04-26 08:12:44.4  |2427749 |true           |\n",
      "|118920064 |5264449|0    |low           |180        |2021-04-27 07:30:19.343|2427749 |true           |\n",
      "|118959104 |5264449|1    |medium        |177        |2021-04-28 13:41:01.027|2427749 |true           |\n",
      "+----------+-------+-----+--------------+-----------+-----------------------+--------+---------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "3. Estadísticas de decodificación:\n",
      "+---------------+------+\n",
      "|is_text_decoded| count|\n",
      "+---------------+------+\n",
      "|           true|100000|\n",
      "+---------------+------+\n",
      "\n",
      "4. Realizando MERGE con 100000 comentarios válidos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tabla 'comments' creada exitosamente\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datos insertados exitosamente\n",
      "5. Verificando datos en Silver...\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|   silver| comments|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_silver_layer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
