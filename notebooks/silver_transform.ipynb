{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb9bbf29-b951-4c32-a6cc-b308f981ef0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b365c5e4-ee2e-4e30-93c6-e25557c0bf32;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.96.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound software.amazon.awssdk#bundle;2.20.18 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.5.2/iceberg-spark-runtime-3.5_2.12-1.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2!iceberg-spark-runtime-3.5_2.12.jar (4864ms)\n",
      "downloading https://repo1.maven.org/maven2/org/projectnessie/nessie-integrations/nessie-spark-extensions-3.5_2.12/0.96.1/nessie-spark-extensions-3.5_2.12-0.96.1.jar ...\n",
      "\t[SUCCESSFUL ] org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.96.1!nessie-spark-extensions-3.5_2.12.jar (454ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.4!hadoop-aws.jar (307ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.20.18/bundle-2.20.18.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#bundle;2.20.18!bundle.jar (45582ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.262!aws-java-sdk-bundle.jar (25032ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (541ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/eventstream/eventstream/1.0.1/eventstream-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.eventstream#eventstream;1.0.1!eventstream.jar (243ms)\n",
      ":: resolution report :: resolve 11378ms :: artifacts dl 77463ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.96.1 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.20.18 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   7   |   7   |   0   ||   7   |   7   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b365c5e4-ee2e-4e30-93c6-e25557c0bf32\n",
      "\tconfs: [default]\n",
      "\t7 artifacts copied, 0 already retrieved (771650kB/26992ms)\n",
      "25/10/15 21:16:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/15 21:16:49 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException: Cannot invoke \"org.apache.spark.storage.BlockManagerId.executorId()\" because \"idWithoutTopologyInfo\" is null\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:677)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/10/15 21:16:49 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1240)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:296)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.storage.BlockManagerId.executorId()\" because \"idWithoutTopologyInfo\" is null\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:677)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "25/10/15 21:16:54 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException: Cannot invoke \"org.apache.spark.storage.BlockManagerId.executorId()\" because \"idWithoutTopologyInfo\" is null\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:677)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/10/15 21:16:54 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1240)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:296)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.storage.BlockManagerId.executorId()\" because \"idWithoutTopologyInfo\" is null\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:677)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sesión Spark optimizada creada exitosamente\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "def create_optimized_silver_spark_session():\n",
    "    \"\"\"\n",
    "    Crea una sesión de Spark optimizada para recursos con configuración robusta\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark = (\n",
    "            SparkSession.builder\n",
    "            .appName(\"SilverLayer-Optimized\")\n",
    "            .config('spark.jars.packages', \n",
    "                    'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,'\n",
    "                    'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.96.1,'\n",
    "                    'org.apache.hadoop:hadoop-aws:3.3.4,'\n",
    "                    'software.amazon.awssdk:bundle:2.20.18')  # Added for better S3 stability\n",
    "            # Configuración Nessie/Iceberg\n",
    "            .config(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "            .config(\"spark.sql.catalog.nessie.uri\", \"http://nessie:19120/api/v1\")\n",
    "            .config(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "            .config(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\")\n",
    "            .config(\"spark.sql.catalog.nessie.warehouse\", \"s3a://lakehouse/\")\n",
    "            .config(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "            .config(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "            \n",
    "            # Configuración S3/MinIO optimizada\n",
    "            .config(\"spark.sql.catalog.nessie.s3.endpoint\", \"http://minio:9000\")\n",
    "            .config(\"spark.sql.catalog.nessie.s3.access-key-id\", \"minioadmin\")\n",
    "            .config(\"spark.sql.catalog.nessie.s3.secret-access-key\", \"minioadmin\")\n",
    "            .config(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "            \n",
    "            # Configuración Hadoop/S3A optimizada\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\")\n",
    "            .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"10\")\n",
    "            .config(\"spark.hadoop.fs.s3a.retry.limit\", \"5\")\n",
    "            \n",
    "            # Extensiones\n",
    "            .config(\"spark.sql.extensions\", \n",
    "                   \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,\"\n",
    "                   \"org.projectnessie.spark.extensions.NessieSparkSessionExtensions\")\n",
    "            \n",
    "            # OPTIMIZACIONES DE MEMORIA Y RECURSOS\n",
    "            .config(\"spark.driver.memory\", \"4g\")           # Aumentado para estabilidad\n",
    "            .config(\"spark.executor.memory\", \"4g\")         # Aumentado para operaciones Iceberg\n",
    "            .config(\"spark.memory.fraction\", \"0.8\")        # Porcentaje de memoria para ejecución\n",
    "            .config(\"spark.memory.storageFraction\", \"0.3\") # Memoria para storage\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\")  # Query execution adaptativo\n",
    "            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "            .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "            \n",
    "            # OPTIMIZACIONES PARA ICEBERG\n",
    "            .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\")\n",
    "            .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "            .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "            .config(\"spark.sql.catalog.nessie.vectorization-enabled\", \"false\")  # Mejor estabilidad\n",
    "            \n",
    "            # GESTIÓN DE CACHE Y SERIALIZACIÓN\n",
    "            .config(\"spark.sql.inMemoryColumnarStorage.compressed\", \"true\")\n",
    "            .config(\"spark.sql.inMemoryColumnarStorage.batchSize\", \"10000\")\n",
    "            .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "            \n",
    "            # MANEJO DE ERRORES Y RECONEXIÓN\n",
    "            .config(\"spark.sql.retainGroupColumns\", \"false\")\n",
    "            .config(\"spark.cleaner.periodicGC.interval\", \"1min\")\n",
    "            .config(\"spark.cleaner.referenceTracking.cleanCheckpoints\", \"true\")\n",
    "            \n",
    "            .getOrCreate()\n",
    "        )\n",
    "        \n",
    "        # Configuración adicional vía SparkContext\n",
    "        spark.sparkContext.setLogLevel(\"WARN\")  # Reducir verbosidad\n",
    "        \n",
    "        print(\"✅ Sesión Spark optimizada creada exitosamente\")\n",
    "        return spark\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creando sesión Spark: {e}\")\n",
    "        raise\n",
    "\n",
    "# Crear sesión optimizada\n",
    "spark = create_optimized_silver_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e184e03-eb55-4463-841c-70ae9eb3c381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROCESO SILVER ===\n",
      "=== CONFIGURANDO NAMESPACES EN NESSIE ===\n",
      "Namespaces existentes:\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "+---------+\n",
      "\n",
      "Creando namespace 'silver'...\n",
      "✅ Namespace 'silver' creado exitosamente\n",
      "Namespaces después de la creación:\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   silver|\n",
      "+---------+\n",
      "\n",
      "\n",
      "--- COMMENTS ---\n",
      "Procesando: manual - comments 2020...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/15 21:17:14 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo de: s3a://lakehouse/bronze/Comments_2020.parquet\n",
      "📑 Columnas normalizadas: ['id', 'post_id', 'score', 'text', 'creation_date', 'user_id', 'user_display_name']\n",
      "✅ Leídos 1000 registros de s3a://lakehouse/bronze/Comments_2020.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ manual 2020: 1000 registros\n",
      "Procesando: dlt - comments 2021...\n",
      "✅ Encontrado archivo DLT: 1760554762.3482103.95c5ec2953.parquet\n",
      "Leyendo de: s3a://lakehouse/bronze/comments_2021/comments_2021/1760554762.3482103.95c5ec2953.parquet\n",
      "📑 Columnas normalizadas: ['id', 'post_id', 'score', 'text', 'creation_date', 'user_id', 'user_display_name']\n",
      "✅ Leídos 1000 registros de s3a://lakehouse/bronze/comments_2021/comments_2021/1760554762.3482103.95c5ec2953.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ dlt 2021: 1000 registros\n",
      "Realizando MERGE en: nessie.silver.comments\n",
      "ℹ  Tabla nessie.silver.comments no existe, se creará\n",
      "Creando nueva tabla Iceberg: nessie.silver.comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                 (0 + 6) / 7][Stage 29:>                 (0 + 0) / 6]\r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import (\n",
    "    lit, col, when, length, to_timestamp, year, month, dayofmonth,\n",
    "    datediff, current_date, udf\n",
    ")\n",
    "from pyspark.sql.types import StringType\n",
    "from datetime import datetime\n",
    "\n",
    "# =========================\n",
    "# Normalización y validación\n",
    "# =========================\n",
    "def _normalize_cols(df):\n",
    "    import re\n",
    "    new_cols = []\n",
    "    for c in df.columns:\n",
    "        # camelCase / PascalCase → snake_case\n",
    "        c2 = re.sub(r'(?<!^)(?=[A-Z])', '_', c)\n",
    "        c2 = c2.replace('-', '').replace(' ', '').lower()\n",
    "\n",
    "        # normalizaciones manuales más comunes según tus archivos\n",
    "        c2 = (\n",
    "            c2.replace('userid', 'user_id')\n",
    "               .replace('postid', 'post_id')\n",
    "               .replace('creationdate', 'creation_date')\n",
    "               .replace('displayname', 'display_name')\n",
    "               .replace('accountid', 'account_id')\n",
    "               .replace('userdisplayname', 'user_display_name')\n",
    "        )\n",
    "        new_cols.append(c2)\n",
    "    return df.toDF(*new_cols)\n",
    "\n",
    "\n",
    "def _validate_schema(df, dataset_name, year, bronze_path):\n",
    "    cols = set(df.columns)\n",
    "    ds = (dataset_name or \"\").lower()\n",
    "\n",
    "    if ds == \"comments\":\n",
    "        required = {\"id\", \"post_id\", \"text\", \"user_id\", \"creation_date\"}\n",
    "    elif ds == \"posts\":\n",
    "        required = {\"id\", \"post_type_id\", \"creation_date\"}\n",
    "    elif ds == \"badges\":\n",
    "        required = {\"id\", \"user_id\", \"name\", \"date\"}\n",
    "    elif ds == \"users\":\n",
    "        required = {\"id\", \"display_name\", \"creation_date\"}\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    missing = required - cols\n",
    "    if missing:\n",
    "        # si faltan, intenta ver si hay equivalentes antes de fallar\n",
    "        alt_map = {\n",
    "            \"user_id\": [\"userid\"],\n",
    "            \"post_id\": [\"postid\"],\n",
    "            \"creation_date\": [\"creationdate\"],\n",
    "            \"display_name\": [\"displayname\"],\n",
    "        }\n",
    "        for req, alts in alt_map.items():\n",
    "            if req in missing and any(a in cols for a in alts):\n",
    "                missing.remove(req)\n",
    "\n",
    "    if missing:\n",
    "        sample_cols = \", \".join(sorted(list(cols))[:20])\n",
    "        raise ValueError(\n",
    "            f\"[{dataset_name} {year}] El archivo no parece de '{dataset_name}'. \"\n",
    "            f\"Faltan columnas: {sorted(list(missing))}. \"\n",
    "            f\"Vistas (muestra): {sample_cols} ... (path: {bronze_path})\"\n",
    "        )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Hadoop FS helpers (path-aware) → evita Wrong FS file:///\n",
    "# =========================\n",
    "def _hconf():\n",
    "    return spark._jsc.hadoopConfiguration()\n",
    "\n",
    "def _jPath(p: str):\n",
    "    return spark._jvm.org.apache.hadoop.fs.Path(p)\n",
    "\n",
    "def _fs_for(p: str):\n",
    "    # * clave: usa el FileSystem del Path (respeta esquema s3a://) *\n",
    "    return _jPath(p).getFileSystem(_hconf())\n",
    "\n",
    "def _exists(p: str) -> bool:\n",
    "    try:    return _fs_for(p).exists(_jPath(p))\n",
    "    except: return False\n",
    "\n",
    "def _is_dir(p: str) -> bool:\n",
    "    try:    return _fs_for(p).isDirectory(_jPath(p))\n",
    "    except: return False\n",
    "\n",
    "def _listdir(p: str):\n",
    "    try:    return _fs_for(p).listStatus(_jPath(p))\n",
    "    except: return []\n",
    "\n",
    "def _parquet_files_in(dir_str: str, limit: int = None):\n",
    "    out = []\n",
    "    for st in _listdir(dir_str):\n",
    "        pp = st.getPath().toString()\n",
    "        if st.isFile() and pp.endswith(\".parquet\"):\n",
    "            out.append(pp)\n",
    "    if limit: out = out[:limit]\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# Resolver robusto de rutas (ajustado a tu bucket)\n",
    "# =========================\n",
    "def _resolve_manual_bronze_path(dataset_name, year=None):\n",
    "    ds = dataset_name.lower()\n",
    "    base = \"s3a://lakehouse/bronze\"\n",
    "\n",
    "    if year is None:\n",
    "        # badges/users en archivo único\n",
    "        candidate_patterns = [\n",
    "            f\"{base}/{ds}.parquet\",              # badges.parquet / users.parquet\n",
    "            f\"{base}/{ds.capitalize()}.parquet\", # por si viene con mayúscula inicial\n",
    "        ]\n",
    "    else:\n",
    "        # 🔹 Permitir variantes singulares/plurales\n",
    "        if ds.endswith(\"s\"):\n",
    "            ds_singular = ds[:-1]\n",
    "        else:\n",
    "            ds_singular = ds\n",
    "\n",
    "        # 🔹 Construcción de patrones posibles\n",
    "        candidate_patterns = [\n",
    "            f\"{base}/{ds.capitalize()}_{year}.parquet\",\n",
    "            f\"{base}/{ds.title()}_{year}.parquet\",\n",
    "            f\"{base}/{ds}_{year}.parquet\",\n",
    "            f\"{base}/{ds}_{year}/\",\n",
    "            # 👇 añade equivalentes singulares\n",
    "            f\"{base}/{ds_singular.capitalize()}_{year}.parquet\",\n",
    "            f\"{base}/{ds_singular}_{year}.parquet\",\n",
    "            f\"{base}/{ds_singular}_{year}/\",\n",
    "        ]\n",
    "\n",
    "    # columnas esperadas para validación rápida\n",
    "    expected = {\n",
    "        \"comments\": {\"id\", \"post_id\", \"text\", \"user_id\", \"creation_date\"},\n",
    "        \"posts\":    {\"id\", \"post_type_id\", \"creation_date\"},\n",
    "        \"badges\":   {\"id\", \"user_id\", \"name\", \"date\"},\n",
    "        \"users\":    {\"id\", \"display_name\", \"creation_date\"},\n",
    "    }.get(ds, set())\n",
    "\n",
    "    tried = []\n",
    "    for cand in candidate_patterns:\n",
    "        if not _exists(cand):\n",
    "            tried.append(f\"(no existe) {cand}\")\n",
    "            continue\n",
    "\n",
    "        if _is_dir(cand):\n",
    "            # carpeta (ej: comments_2021/) → busca .parquet dentro\n",
    "            parquet_list = _parquet_files_in(cand, limit=50)\n",
    "            if not parquet_list:\n",
    "                tried.append(f\"(carpeta sin .parquet) {cand}\")\n",
    "                continue\n",
    "\n",
    "            for pq in parquet_list:\n",
    "                try:\n",
    "                    df_try = spark.read.parquet(pq)\n",
    "                    df_try = _normalize_cols(df_try)\n",
    "                    cols = set(df_try.columns)\n",
    "                    if expected.issubset(cols):\n",
    "                        _validate_schema(df_try, ds, year, pq)\n",
    "                        return pq\n",
    "                    else:\n",
    "                        tried.append(f\"(mismatch schema en {pq}, cols: {sorted(list(cols))[:12]})\")\n",
    "                except Exception as e:\n",
    "                    tried.append(f\"(error leyendo {pq}: {e})\")\n",
    "        else:\n",
    "            # archivo directo\n",
    "            try:\n",
    "                df_try = spark.read.parquet(cand)\n",
    "                df_try = _normalize_cols(df_try)\n",
    "                cols = set(df_try.columns)\n",
    "                if expected.issubset(cols):\n",
    "                    _validate_schema(df_try, ds, year, cand)\n",
    "                    return cand\n",
    "                else:\n",
    "                    tried.append(f\"(mismatch schema en {cand}, cols: {sorted(list(cols))[:12]})\")\n",
    "            except Exception as e:\n",
    "                tried.append(f\"(error leyendo {cand}: {e})\")\n",
    "\n",
    "    details = \"\\n  - \".join(tried) if tried else \"(sin intentos)\"\n",
    "    raise FileNotFoundError(\n",
    "        f\"No pude resolver una ruta válida para dataset='{dataset_name}', year={year} en {base}.\\n\"\n",
    "        f\"Intentos:\\n  - {details}\\n\"\n",
    "        f\"Verifica nombres y que existan los archivos en el bucket.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Lector principal\n",
    "# =========================\n",
    "def read_bronze_data(source_type=\"manual\", dataset_name=None, year=None, limit=None):\n",
    "    \"\"\"\n",
    "    Lee datos Bronze desde S3A (MinIO/AWS), normaliza columnas y valida esquema.\n",
    "    \"\"\"\n",
    "    ds = (dataset_name or \"\").lower()\n",
    "\n",
    "    if source_type == \"manual\":\n",
    "        if ds in [\"comments\", \"post\", \"posts\"] and year:\n",
    "            bronze_path = _resolve_manual_bronze_path(ds, year=year)\n",
    "        elif ds in [\"badges\", \"users\"] and year is None:\n",
    "            bronze_path = _resolve_manual_bronze_path(ds, year=None)\n",
    "        else:\n",
    "            raise ValueError(f\"Combinación no válida: dataset={dataset_name}, year={year}\")\n",
    "\n",
    "    elif source_type == \"dlt\":\n",
    "        if ds == \"comments\" and year == 2021:\n",
    "            base_dir = \"s3a://lakehouse/bronze/comments_2021/comments_2021/\"\n",
    "            if not _exists(base_dir) or not _is_dir(base_dir):\n",
    "                raise FileNotFoundError(f\"No existe carpeta DLT: {base_dir}\")\n",
    "            cands = _parquet_files_in(base_dir, limit=100)\n",
    "            if not cands:\n",
    "                raise FileNotFoundError(f\"No hay .parquet dentro de {base_dir}\")\n",
    "            bronze_path = None\n",
    "            last_errs = []\n",
    "            for pq in cands:\n",
    "                try:\n",
    "                    df_try = spark.read.parquet(pq)\n",
    "                    df_try = _normalize_cols(df_try)\n",
    "                    _validate_schema(df_try, ds, year, pq)\n",
    "                    bronze_path = pq\n",
    "                    print(f\"✅ Encontrado archivo DLT: {pq.split('/')[-1]}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    last_errs.append(str(e))\n",
    "            if bronze_path is None:\n",
    "                raise ValueError(f\"No se halló parquet válido en {base_dir}. Errores: {last_errs[:3]}\")\n",
    "        else:\n",
    "            raise ValueError(\"DLT solo disponible para comments 2021 en este flujo.\")\n",
    "    else:\n",
    "        raise ValueError(\"source_type debe ser 'manual' o 'dlt'\")\n",
    "\n",
    "    print(f\"Leyendo de: {bronze_path}\")\n",
    "    try:\n",
    "        df = spark.read.parquet(bronze_path)\n",
    "        df = _normalize_cols(df)\n",
    "        print(f\"📑 Columnas normalizadas: {df.columns}\")\n",
    "        _validate_schema(df, dataset_name, year, bronze_path)\n",
    "        if limit:\n",
    "            df = df.limit(limit)\n",
    "        print(f\"✅ Leídos {df.count()} registros de {bronze_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error leyendo {bronze_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# =========================\n",
    "# UDF para decodificar binarios\n",
    "# =========================\n",
    "def binary_to_utf8(binary_data):\n",
    "    try:\n",
    "        if binary_data is None:\n",
    "            return None\n",
    "        return binary_data.decode('utf-8')\n",
    "    except Exception as e:\n",
    "        return f\"[DECODE_ERROR: {str(e)}]\"\n",
    "\n",
    "# ⚠ CORRECCIÓN: Faltaba el paréntesis de cierre\n",
    "binary_to_utf8_udf = udf(binary_to_utf8, StringType())\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Setup Nessie\n",
    "# =========================\n",
    "def setup_nessie_namespaces():\n",
    "    \"\"\"Crear los namespaces necesarios en Nessie\"\"\"\n",
    "    print(\"=== CONFIGURANDO NAMESPACES EN NESSIE ===\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Namespaces existentes:\")\n",
    "        spark.sql(\"SHOW NAMESPACES IN nessie\").show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error mostrando namespaces: {e}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Creando namespace 'silver'...\")\n",
    "        spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.silver\")\n",
    "        print(\"✅ Namespace 'silver' creado exitosamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creando namespace silver: {e}\")\n",
    "        try:\n",
    "            spark.sql(\"CREATE SCHEMA IF NOT EXISTS nessie.silver\")\n",
    "            print(\"✅ Schema 'silver' creado exitosamente\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Error creando schema: {e2}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Namespaces después de la creación:\")\n",
    "        spark.sql(\"SHOW NAMESPACES IN nessie\").show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error verificando namespaces: {e}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Schema vacío para comments\n",
    "# =========================\n",
    "def create_empty_comments_df():\n",
    "    \"\"\"Schema actualizado para comments con TODAS las columnas\"\"\"\n",
    "    from pyspark.sql.types import StructType, StructField, LongType, StringType, IntegerType, BooleanType, TimestampType\n",
    "    \n",
    "    schema = StructType([\n",
    "        # Bronze (7)\n",
    "        StructField(\"comment_id\", LongType(), True),\n",
    "        StructField(\"post_id\", LongType(), True),\n",
    "        StructField(\"score\", LongType(), True),\n",
    "        StructField(\"comment_text\", StringType(), True),\n",
    "        StructField(\"creation_date\", TimestampType(), True),\n",
    "        StructField(\"user_id\", LongType(), True),\n",
    "        StructField(\"user_display_name\", StringType(), True),\n",
    "        # Enriquecimiento (7)\n",
    "        StructField(\"score_category\", StringType(), True),\n",
    "        StructField(\"text_length\", IntegerType(), True),\n",
    "        StructField(\"comment_year\", IntegerType(), True),\n",
    "        StructField(\"comment_month\", IntegerType(), True),\n",
    "        StructField(\"comment_day\", IntegerType(), True),\n",
    "        StructField(\"has_user_display_name\", BooleanType(), True),\n",
    "        StructField(\"load_date\", TimestampType(), True)\n",
    "    ])\n",
    "    \n",
    "    return spark.createDataFrame([], schema)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# TRANSFORMACIONES SILVER\n",
    "# =========================\n",
    "\n",
    "def transform_comments_to_silver_with_year(df, year, load_timestamp):\n",
    "    \"\"\"Transforma comments con TODAS las columnas (7 Bronze + 7 Enriquecimiento)\"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .withColumnRenamed(\"id\", \"comment_id\")\n",
    "        .withColumnRenamed(\"text\", \"comment_text_binary\")\n",
    "        .withColumnRenamed(\"user_display_name\", \"user_display_name_binary\")\n",
    "        .withColumn(\"comment_text\", binary_to_utf8_udf(col(\"comment_text_binary\")))\n",
    "        .withColumn(\"user_display_name\", binary_to_utf8_udf(col(\"user_display_name_binary\")))\n",
    "        .withColumn(\"score_category\",\n",
    "                   when(col(\"score\") >= 5, \"high\")\n",
    "                   .when(col(\"score\") >= 1, \"medium\")\n",
    "                   .otherwise(\"low\"))\n",
    "        .withColumn(\"text_length\", \n",
    "                   when(col(\"comment_text\").isNotNull(), length(col(\"comment_text\")))\n",
    "                   .otherwise(0))\n",
    "        .withColumn(\"has_user_display_name\", \n",
    "                   when(col(\"user_display_name\").isNull() | \n",
    "                        (col(\"user_display_name\") == \"\"), \n",
    "                        False).otherwise(True))\n",
    "        .withColumn(\"comment_year\", lit(year))\n",
    "        .withColumn(\"comment_month\", month(to_timestamp(col(\"creation_date\"))))\n",
    "        .withColumn(\"comment_day\", dayofmonth(to_timestamp(col(\"creation_date\"))))\n",
    "        .withColumn(\"load_date\", lit(load_timestamp).cast(\"timestamp\"))\n",
    "        .withColumn(\"is_text_decoded\", ~col(\"comment_text\").contains(\"[DECODE_ERROR]\"))\n",
    "        .filter(col(\"is_text_decoded\") == True)\n",
    "        .select(\n",
    "            \"comment_id\", \"post_id\", \"score\", \"comment_text\", \"creation_date\",\n",
    "            \"user_id\", \"user_display_name\", \"score_category\", \"text_length\",\n",
    "            \"comment_year\", \"comment_month\", \"comment_day\",\n",
    "            \"has_user_display_name\", \"load_date\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def transform_badges_to_silver(badges_df, load_timestamp):\n",
    "    \"\"\"Transforma badges con TODAS las columnas (6 Bronze + 3 Enriquecimiento)\"\"\"\n",
    "    return (\n",
    "        badges_df\n",
    "        .withColumnRenamed(\"id\", \"badge_id\")\n",
    "        .withColumnRenamed(\"name\", \"badge_name\")\n",
    "        .withColumnRenamed(\"date\", \"award_date\")\n",
    "        .withColumnRenamed(\"class\", \"badge_class\")\n",
    "        .withColumnRenamed(\"tag_based\", \"is_tag_based\")\n",
    "        .withColumn(\"badge_year\", year(to_timestamp(col(\"award_date\"))))\n",
    "        .withColumn(\"badge_month\", month(to_timestamp(col(\"award_date\"))))\n",
    "        .withColumn(\"load_date\", lit(load_timestamp).cast(\"timestamp\"))\n",
    "        .withColumn(\"badge_name\", binary_to_utf8_udf(col(\"badge_name\")))\n",
    "        .select(\n",
    "            \"badge_id\", \"user_id\", \"badge_name\", \"award_date\",\n",
    "            \"badge_class\", \"is_tag_based\", \"badge_year\", \"badge_month\", \"load_date\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def transform_users_to_silver(users_df, load_timestamp):\n",
    "    \"\"\"Transforma users con TODAS las columnas (12 Bronze + 3 Enriquecimiento)\"\"\"\n",
    "    return (\n",
    "        users_df\n",
    "        .withColumnRenamed(\"id\", \"user_id\")\n",
    "        .withColumn(\"user_age_days\", \n",
    "                   datediff(current_date(), to_timestamp(col(\"creation_date\"))))\n",
    "        .withColumn(\"is_active\", \n",
    "                   datediff(current_date(), to_timestamp(col(\"last_access_date\"))) <= 365)\n",
    "        .withColumn(\"load_date\", lit(load_timestamp).cast(\"timestamp\"))\n",
    "        .withColumn(\"display_name\", binary_to_utf8_udf(col(\"display_name\")))\n",
    "        .withColumn(\"about_me\", binary_to_utf8_udf(col(\"about_me\")))\n",
    "        .withColumn(\"website_url\", binary_to_utf8_udf(col(\"website_url\")))\n",
    "        .withColumn(\"location\", binary_to_utf8_udf(col(\"location\")))\n",
    "        .select(\n",
    "            \"user_id\", \"reputation\", \"creation_date\", \"display_name\",\n",
    "            \"last_access_date\", \"about_me\", \"views\", \"up_votes\", \"down_votes\",\n",
    "            \"website_url\", \"location\", \"account_id\",\n",
    "            \"user_age_days\", \"is_active\", \"load_date\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def transform_posts_to_silver(posts_df, year, load_timestamp):\n",
    "    \"\"\"Transforma posts con TODAS las columnas (21 Bronze + 4 Enriquecimiento)\"\"\"\n",
    "    return (\n",
    "        posts_df\n",
    "        .withColumnRenamed(\"id\", \"post_id\")\n",
    "        .withColumn(\"post_year\", lit(year))\n",
    "        .withColumn(\"post_month\", month(to_timestamp(col(\"creation_date\"))))\n",
    "        .withColumn(\"post_day\", dayofmonth(to_timestamp(col(\"creation_date\"))))\n",
    "        .withColumn(\"load_date\", lit(load_timestamp).cast(\"timestamp\"))\n",
    "        .withColumn(\"body\", binary_to_utf8_udf(col(\"body\")))\n",
    "        .withColumn(\"title\", binary_to_utf8_udf(col(\"title\")))\n",
    "        .withColumn(\"tags\", binary_to_utf8_udf(col(\"tags\")))\n",
    "        .withColumn(\"owner_display_name\", binary_to_utf8_udf(col(\"owner_display_name\")))\n",
    "        .withColumn(\"last_editor_display_name\", binary_to_utf8_udf(col(\"last_editor_display_name\")))\n",
    "        .withColumn(\"content_license\", binary_to_utf8_udf(col(\"content_license\")))\n",
    "        .select(\n",
    "            \"post_id\", \"post_type_id\", \"accepted_answer_id\", \"creation_date\",\n",
    "            \"score\", \"view_count\", \"body\", \"owner_user_id\", \"owner_display_name\",\n",
    "            \"last_editor_user_id\", \"last_editor_display_name\", \"last_edit_date\",\n",
    "            \"last_activity_date\", \"title\", \"tags\", \"answer_count\", \"comment_count\",\n",
    "            \"favorite_count\", \"content_license\", \"parent_id\", \"community_owned_date\",\n",
    "            \"closed_date\", \"post_year\", \"post_month\", \"post_day\", \"load_date\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PROCESAMIENTO\n",
    "# =========================\n",
    "\n",
    "def transform_multiple_years():\n",
    "    \"\"\"Transforma datos de comments de múltiples fuentes y años\"\"\"\n",
    "    limit_per_source = 1000\n",
    "    current_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    all_comments = None\n",
    "    \n",
    "    data_sources = [\n",
    "        {\"type\": \"manual\", \"dataset\": \"comments\", \"year\": 2020},\n",
    "        {\"type\": \"dlt\", \"dataset\": \"comments\", \"year\": 2021},\n",
    "    ]\n",
    "    \n",
    "    for source in data_sources:\n",
    "        print(f\"Procesando: {source['type']} - {source['dataset']} {source['year']}...\")\n",
    "        try:\n",
    "            source_data = read_bronze_data(\n",
    "                source_type=source['type'],\n",
    "                dataset_name=source['dataset'], \n",
    "                year=source['year'],\n",
    "                limit=limit_per_source\n",
    "            )\n",
    "            if source_data.count() > 0:\n",
    "                source_transformed = transform_comments_to_silver_with_year(\n",
    "                    source_data, source['year'], current_timestamp\n",
    "                )\n",
    "                if all_comments is None:\n",
    "                    all_comments = source_transformed\n",
    "                else:\n",
    "                    all_comments = all_comments.union(source_transformed)\n",
    "                print(f\"  ✅ {source['type']} {source['year']}: {source_transformed.count()} registros\")\n",
    "            else:\n",
    "                print(f\"  ⚠  {source['type']} {source['year']}: Sin datos\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error procesando {source['type']} {source['year']}: {e}\")\n",
    "    \n",
    "    if all_comments is None:\n",
    "        print(\"⚠  No se pudieron procesar fuentes, creando DataFrame vacío\")\n",
    "        all_comments = create_empty_comments_df()\n",
    "    \n",
    "    return all_comments\n",
    "\n",
    "\n",
    "def merge_into_silver_table_compliant(silver_df, table_name, key_columns):\n",
    "    \"\"\"MERGE real que maneja duplicados y preserva históricos\"\"\"\n",
    "    silver_table_path = f\"nessie.silver.{table_name}\"\n",
    "    print(f\"Realizando MERGE en: {silver_table_path}\")\n",
    "    \n",
    "    try:\n",
    "        spark.sql(f\"DESCRIBE {silver_table_path}\").show()\n",
    "        table_exists = True\n",
    "        print(f\"✅ Tabla {silver_table_path} existe\")\n",
    "    except:\n",
    "        table_exists = False\n",
    "        print(f\"ℹ  Tabla {silver_table_path} no existe, se creará\")\n",
    "    \n",
    "    if not table_exists:\n",
    "        print(f\"Creando nueva tabla Iceberg: {silver_table_path}\")\n",
    "        (silver_df\n",
    "         .writeTo(silver_table_path)\n",
    "         .using(\"iceberg\")\n",
    "         .tableProperty(\"format-version\", \"2\")\n",
    "         .tableProperty(\"write.update.mode\", \"merge-on-read\")\n",
    "         .tableProperty(\"write.merge.mode\", \"merge-on-read\")\n",
    "         .create())\n",
    "        print(f\"✅ Tabla creada con {silver_df.count()} registros iniciales\")\n",
    "    else:\n",
    "        print(f\"Realizando MERGE...\")\n",
    "        silver_df.createOrReplaceTempView(\"new_data\")\n",
    "        join_condition = ' AND '.join([f'target.{col} = source.{col}' for col in key_columns])\n",
    "        merge_sql = f\"\"\"\n",
    "        MERGE INTO {silver_table_path} AS target\n",
    "        USING new_data AS source\n",
    "        ON {join_condition}\n",
    "        WHEN MATCHED THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "        spark.sql(merge_sql)\n",
    "        print(\"✅ MERGE completado\")\n",
    "        final_count = spark.sql(f\"SELECT COUNT(*) as total FROM {silver_table_path}\").collect()[0]['total']\n",
    "        print(f\"📊 Total registros: {final_count}\")\n",
    "    \n",
    "    print(f\"\\n📸 Snapshots de {table_name}:\")\n",
    "    snapshots_df = spark.sql(f\"\"\"\n",
    "        SELECT snapshot_id, committed_at, operation, \n",
    "               summary['added-records'] as added_records,\n",
    "               summary['deleted-records'] as deleted_records\n",
    "        FROM {silver_table_path}.snapshots \n",
    "        ORDER BY committed_at DESC\n",
    "    \"\"\")\n",
    "    snapshots_df.show(truncate=False)\n",
    "    return snapshots_df.count()\n",
    "\n",
    "\n",
    "def transform_other_datasets():\n",
    "    \"\"\"Transforma badges, users, posts a Silver\"\"\"\n",
    "    current_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Procesando badges...\")\n",
    "        badges_df = read_bronze_data(source_type=\"manual\", dataset_name=\"badges\", limit=1000)\n",
    "        silver_badges = transform_badges_to_silver(badges_df, current_timestamp)\n",
    "        merge_into_silver_table_compliant(silver_badges, \"badges\", [\"badge_id\"])\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error badges: {e}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Procesando users...\")\n",
    "        users_df = read_bronze_data(source_type=\"manual\", dataset_name=\"users\", limit=1000)\n",
    "        silver_users = transform_users_to_silver(users_df, current_timestamp)\n",
    "        merge_into_silver_table_compliant(silver_users, \"users\", [\"user_id\"])\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error users: {e}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Procesando posts...\")\n",
    "        posts_years = [2020, 2021]\n",
    "        all_posts = None\n",
    "        for year in posts_years:\n",
    "            try:\n",
    "                posts_df = read_bronze_data(source_type=\"manual\", dataset_name=\"posts\", year=year, limit=1000)\n",
    "                posts_transformed = transform_posts_to_silver(posts_df, year, current_timestamp)\n",
    "                if all_posts is None:\n",
    "                    all_posts = posts_transformed\n",
    "                else:\n",
    "                    all_posts = all_posts.union(posts_transformed)\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Error posts {year}: {e}\")\n",
    "        if all_posts and all_posts.count() > 0:\n",
    "            merge_into_silver_table_compliant(all_posts, \"posts\", [\"post_id\"])\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error posts: {e}\")\n",
    "\n",
    "\n",
    "def verify_silver_compliance():\n",
    "    \"\"\"Verifica cumplimiento de requisitos Silver\"\"\"\n",
    "    print(\"=== VERIFICACIÓN SILVER ===\")\n",
    "    for table in [\"comments\", \"badges\", \"users\", \"posts\"]:\n",
    "        print(f\"\\n📋 {table}:\")\n",
    "        try:\n",
    "            spark.sql(f\"DESCRIBE nessie.silver.{table}\").show()\n",
    "            snapshots = spark.sql(f\"SELECT COUNT(*) as c FROM nessie.silver.{table}.snapshots\").collect()\n",
    "            schema = spark.sql(f\"DESCRIBE nessie.silver.{table}\").collect()\n",
    "            count = spark.sql(f\"SELECT COUNT(*) as c FROM nessie.silver.{table}\").collect()\n",
    "            print(f\"  ✅ Snapshots: {snapshots[0]['c']}\")\n",
    "            print(f\"  ✅ load_date: {'SÍ' if any('load_date' in str(r) for r in schema) else 'NO'}\")\n",
    "            print(f\"  ✅ Registros: {count[0]['c']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error: {e}\")\n",
    "\n",
    "\n",
    "def test_merge_functionality():\n",
    "    \"\"\"Verifica que el MERGE funciona\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PRUEBA DE MERGE\")\n",
    "    print(\"=\"*60)\n",
    "    for table in [\"comments\", \"badges\", \"users\", \"posts\"]:\n",
    "        print(f\"\\n📋 {table}:\")\n",
    "        try:\n",
    "            snapshots = spark.sql(f\"\"\"\n",
    "                SELECT snapshot_id, committed_at, operation,\n",
    "                       summary['added-records'] as added,\n",
    "                       summary['total-records'] as total\n",
    "                FROM nessie.silver.{table}.snapshots\n",
    "                ORDER BY committed_at\n",
    "            \"\"\")\n",
    "            count = snapshots.count()\n",
    "            print(f\"  📸 Snapshots: {count}\")\n",
    "            if count > 1:\n",
    "                print(f\"  ✅ MERGE funcionando\")\n",
    "            else:\n",
    "                print(f\"  ⚠  Ejecuta nuevamente para probar MERGE\")\n",
    "            snapshots.show(truncate=False)\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error: {e}\")\n",
    "\n",
    "\n",
    "def process_silver_layer_complete_fixed():\n",
    "    \"\"\"Proceso Silver completo\"\"\"\n",
    "    print(\"=== PROCESO SILVER ===\")\n",
    "    setup_nessie_namespaces()\n",
    "    \n",
    "    print(\"\\n--- COMMENTS ---\")\n",
    "    silver_comments = transform_multiple_years()\n",
    "    merge_into_silver_table_compliant(silver_comments, \"comments\", [\"comment_id\"])\n",
    "    \n",
    "    print(\"\\n--- OTROS DATASETS ---\")\n",
    "    transform_other_datasets()\n",
    "    \n",
    "    print(\"\\n--- VERIFICACIÓN ---\")\n",
    "    verify_silver_compliance()\n",
    "    \n",
    "    print(\"\\n=== COMPLETADO ===\")\n",
    "    return silver_comments\n",
    "\n",
    "\n",
    "# Ejecutar\n",
    "process_silver_layer_complete_fixed()\n",
    "test_merge_functionality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41bf33d-788d-49fd-8d24-ae6eac098521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
