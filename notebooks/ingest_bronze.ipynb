{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d88a0cd2-2c32-4f55-b30a-9b64a873dbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-21.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03fdaeb0-5ccf-4eb2-b658-72fe803edfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m9.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, pandas\n",
      "Successfully installed numpy-2.3.3 pandas-2.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "018ff980-a832-428a-8d66-dce5346cf734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectado correctamente a MinIO\n",
      "Buckets encontrados: ['lakehouse', 'mybucket']\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import EndpointConnectionError, ClientError\n",
    "\n",
    "# Aquí define tus credenciales para revisar si conecta Jupyter con Minio. ¡Importante hacer esto para estar seguro de tus rutas!\n",
    "ENDPOINT = \"http://minio:9000\"\n",
    "ACCESS   = \"minioadmin\"\n",
    "SECRET   = \"minioadmin\"\n",
    "\n",
    "try:\n",
    "    s3 = boto3.client(\"s3\",\n",
    "                      endpoint_url=ENDPOINT,\n",
    "                      aws_access_key_id=ACCESS,\n",
    "                      aws_secret_access_key=SECRET)\n",
    "    response = s3.list_buckets()\n",
    "    response = s3.list_buckets()\n",
    "    print(\"Conectado correctamente a MinIO\")\n",
    "    print(\"Buckets encontrados:\", [b['Name'] for b in response.get('Buckets', [])])\n",
    "except EndpointConnectionError as e:\n",
    "    print(\"¡ERROR! No se pudo conectar al endpoint:\", e)\n",
    "except ClientError as e:\n",
    "    print(\"¡OJO! Error de cliente S3:\", e)\n",
    "except Exception as e:\n",
    "    print(\"Otro error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba7c251-c434-469b-9dc6-4d059a7fb994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrados 6 archivos parquet en /data/StackOverflowData\n",
      "[1/6] Subiendo badges.parquet a MinIO ... LISTO!\n",
      "[2/6] Subiendo Comments2020.parquet a MinIO ... LISTO!\n",
      "[3/6] Saltando Comments2021.parquet (se cargará con DLT)\n",
      "[4/6] Subiendo Posts2020.parquet a MinIO ... LISTO!\n",
      "[5/6] Subiendo Posts2021.parquet a MinIO ... LISTO!\n",
      "[6/6] Subiendo users.parquet a MinIO ... "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://minio:9000\", \n",
    "    aws_access_key_id=\"minioadmin\",\n",
    "    aws_secret_access_key=\"minioadmin\"\n",
    ")\n",
    "\n",
    "bucket = \"lakehouse\"\n",
    "base_path = \"/data/StackOverflowData\"  # carpeta dentro DEL CONTENEDOR JUPYTER\n",
    "\n",
    "# Verificación por si la ruta no exite.\n",
    "if not os.path.exists(base_path):\n",
    "    raise FileNotFoundError(f\"No se encuentra la ruta {base_path}. Verifica el volumen en docker-compose.\")\n",
    "\n",
    "# Crear bronze por si no existe.\n",
    "prefix = \"bronze/\"\n",
    "try:\n",
    "    s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=1)\n",
    "except ClientError as e:\n",
    "    print(\"Error al acceder al bucket:\", e)\n",
    "\n",
    "# Subir todo lo que sea .parquet! AVISO: Tarda su tiempo todo depende de los parquets seleccionados.\n",
    "EXCLUIR = {\"Comments2021.parquet\"} \n",
    "\n",
    "files = [f for f in os.listdir(base_path) if f.endswith(\".parquet\")]\n",
    "print(f\"Encontrados {len(files)} archivos parquet en {base_path}\")\n",
    "\n",
    "for i, file in enumerate(files, 1):\n",
    "    if file in EXCLUIR:\n",
    "        print(f\"[{i}/{len(files)}] Saltando {file} (se cargará con DLT)\")\n",
    "        continue  # pasa al siguiente archivo\n",
    "\n",
    "    src = os.path.join(base_path, file)\n",
    "    dest = f\"{prefix}{file}\"\n",
    "    print(f\"[{i}/{len(files)}] Subiendo {file} a MinIO ...\", end=\" \")\n",
    "    s3.upload_file(src, bucket, dest)\n",
    "    print(\"LISTO!\")\n",
    "\n",
    "print(\"Ingesta Bronze completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20db338-6e2a-4767-9538-9c43b12d262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt, pandas as pd\n",
    "\n",
    "@dlt.resource(name=\"posts_2021\")\n",
    "def load_comments_2021():\n",
    "    df = pd.read_parquet(\"/data/StackOverflowData/Comments2021.parquet\")\n",
    "    for rec in df.to_dict(orient=\"records\"):\n",
    "        yield rec  # ¡item por item!\n",
    "\n",
    "pipe = dlt.pipeline(\n",
    "    pipeline_name=\"bronze_ingest_dlt\",\n",
    "    destination=\"filesystem\",              # primero prueba local\n",
    "    dataset_name=\"bronze_posts_2021\",\n",
    "    dev_mode=True,\n",
    "    pipelines_dir=\"/workspace/.dlt\",\n",
    ")\n",
    "\n",
    "print(pipe.run(load_comments_2021()))\n",
    "print(\"Salida local:\", pipe.dataset_path())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d91fbe-5371-4dc7-ac4b-dc59a38fe1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import pyarrow.parquet as pq\n",
    "import dlt\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "URL = \"https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/posts/2021.parquet\"\n",
    "\n",
    "# MinIO (ajusta si corres fuera del contenedor)\n",
    "MINIO_ENDPOINT = os.environ.get(\"MINIO_ENDPOINT\", \"http://minio:9000\")\n",
    "MINIO_ACCESS   = os.environ.get(\"MINIO_ACCESS_KEY_ID\", \"minioadmin\")\n",
    "MINIO_SECRET   = os.environ.get(\"MINIO_SECRET_ACCESS_KEY\", \"minioadmin\")\n",
    "BUCKET         = os.environ.get(\"MINIO_BUCKET\", \"lakehouse\")\n",
    "PREFIX         = \"bronze/posts/2021/\"   # destino lógico en Bronze\n",
    "\n",
    "# DLT: rutas y opciones\n",
    "PIPELINE_NAME  = \"bronze_ingest_dlt_posts2021\"\n",
    "DATASET_NAME   = \"yellow_trip\"          # nombre lógico del dataset en filesystem (no afecta MinIO)\n",
    "PIPELINES_DIR  = \"/workspace/.dlt\"      # estado de dlt en disco\n",
    "\n",
    "\n",
    "@dlt.resource(name=\"posts_2021\", write_disposition=\"replace\")\n",
    "def posts_2021_arrow():\n",
    "    \"\"\"Descarga el Parquet remoto y lo entrega como Arrow Table (streaming seguro).\"\"\"\n",
    "    print(f\"Descargando: {URL}\")\n",
    "    with requests.get(URL, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".parquet\") as tmp:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                if chunk:\n",
    "                    tmp.write(chunk)\n",
    "            tmp.flush()\n",
    "            table = pq.read_table(tmp.name)  # pyarrow.Table\n",
    "            print(f\"Arrow OK: {table.num_rows} filas, {table.num_columns} columnas\")\n",
    "            # DLT acepta pyarrow.Table; si tu versión no, descomenta el yield de dicts:\n",
    "            # for rec in table.to_pylist(): yield rec\n",
    "            yield table\n",
    "\n",
    "\n",
    "def ensure_bucket(s3, bucket: str):\n",
    "    try:\n",
    "        s3.head_bucket(Bucket=bucket)\n",
    "        print(f\"Bucket '{bucket}' existe.\")\n",
    "    except ClientError:\n",
    "        s3.create_bucket(Bucket=bucket)\n",
    "        print(f\"Bucket '{bucket}' creado.\")\n",
    "\n",
    "def upload_folder_to_minio(local_folder: str, bucket: str, prefix: str):\n",
    "    \"\"\"Sube todo el contenido generado por DLT (parquet/metadata) bajo bronze/posts/2021/\"\"\"\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=MINIO_ENDPOINT,\n",
    "        aws_access_key_id=MINIO_ACCESS,\n",
    "        aws_secret_access_key=MINIO_SECRET,\n",
    "    )\n",
    "    ensure_bucket(s3, bucket)\n",
    "\n",
    "\n",
    "    if not prefix.endswith(\"/\"):\n",
    "        prefix += \"/\"\n",
    "    s3.put_object(Bucket=bucket, Key=prefix)\n",
    "\n",
    "    uploaded = 0\n",
    "    for root, _, files in os.walk(local_folder):\n",
    "        for fname in files:\n",
    "            local_path = os.path.join(root, fname)\n",
    "            # clave relativa (conserva estructura de salida de DLT)\n",
    "            rel = os.path.relpath(local_path, local_folder).replace(\"\\\\\", \"/\")\n",
    "            s3_key = f\"{prefix}{rel}\"\n",
    "            print(f\"{local_path} → s3://{bucket}/{s3_key}\")\n",
    "            s3.upload_file(local_path, bucket, s3_key)\n",
    "            uploaded += 1\n",
    "    print(f\"Subidos {uploaded} archivos a s3://{bucket}/{prefix}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        destination=\"filesystem\",           # primero a disco\n",
    "        dataset_name=DATASET_NAME,\n",
    "        full_refresh=True,                  # reemplaza dataset en cada corrida\n",
    "        pipelines_dir=PIPELINES_DIR,\n",
    "        dev_mode=False,\n",
    "    )\n",
    "\n",
    "    print(\"Iniciando ingesta DLT (posts 2021)...\")\n",
    "    load_info = pipeline.run(\n",
    "        posts_2021_arrow(),\n",
    "        loader_file_format=\"parquet\",       # fuerza parquet en filesystem\n",
    "    )\n",
    "    print(\"DLT finalizado.\")\n",
    "    print(load_info)\n",
    "\n",
    "    # 2) Subir a MinIO → lakehouse/bronze/posts/2021/\n",
    "    output_dir = pipeline.dataset_path()    # carpeta donde DLT escribió parquet\n",
    "    print(f\"Output local DLT: {output_dir}\")\n",
    "    upload_folder_to_minio(output_dir, BUCKET, PREFIX)\n",
    "    print(\"Proceso completado.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
