# **Proyecto de Arquitectura Lakehouse y Pipeline de Datos**

## **Descripci√≥n**

Este proyecto implementa una arquitectura **Lakehouse** con un pipeline de procesamiento de datos utilizando herramientas y frameworks de big data. El pipeline procesa los datos a trav√©s de tres etapas: **Bronze**, **Silver** y **Gold**, asegurando calidad de los datos, transformaci√≥n y agregaci√≥n.

La arquitectura emplea las siguientes tecnolog√≠as:

- **MinIO** para almacenamiento de objetos
- **Apache Iceberg** para formatos de tablas
- **Apache Spark** (PySpark) para procesamiento de datos
- **Nessie** como cat√°logo de metadatos
- **Apache Airflow** para orquestaci√≥n de pipelines
- **DLT (Data Load Tool)** para ingesta de datos
- **Dremio/Trino** para consultas SQL
- **Jupyter Notebooks** para ejecuci√≥n manual

## **Estructura del Proyecto**

```plaintext
.
‚îú‚îÄ‚îÄ dags/                         # DAGs de Airflow para las tareas del pipeline
‚îú‚îÄ‚îÄ notebooks/                    # Notebooks de Jupyter para ejecuci√≥n manual
‚îú‚îÄ‚îÄ .gitignore                    # Archivo de exclusi√≥n para Git
‚îú‚îÄ‚îÄ Dockerfile.airflow            # Dockerfile para la configuraci√≥n de Airflow
‚îú‚îÄ‚îÄ Dockerfile.jupyter            # Dockerfile para la configuraci√≥n de Jupyter
‚îú‚îÄ‚îÄ docker-compose.yml            # Archivo Docker Compose para desplegar los contenedores
‚îú‚îÄ‚îÄ requirements.txt              # Dependencias de Python necesarias

```

# **Flujo del Proyecto**

### **1. Bronze**

- **Objetivo**: Ingesta de datos sin transformar y almacenamiento en formato Parquet.
- **Ejecuci√≥n**: Se cargan los datos de las tablas **posts** y **users** de 2020 y 2021 utilizando DLT. Las dem√°s tablas deben cargarse manualmente.
- **Tecnolog√≠as**: DLT, MinIO, formato Parquet.

### **2. Silver**

- **Objetivo**: Transformaci√≥n y normalizaci√≥n de los datos hist√≥ricos utilizando el formato Iceberg.
- **Ejecuci√≥n**: Los datos hist√≥ricos de las tablas **posts** y **users** de 2020 y 2021 se procesan y se fusionan.
- **Tecnolog√≠as**: Apache Spark (PySpark), Apache Iceberg.

### **3. Gold**

- **Objetivo**: Agregaci√≥n de datos y generaci√≥n de m√©tricas y KPIs.
- **Ejecuci√≥n**: Se generan m√©tricas como `post_counts_by_user` y `vote_stats_per_post`.
- **Tecnolog√≠as**: Apache Spark (PySpark), Apache Iceberg.

## **C√≥mo Ejecutar el Proyecto**

Para ejecutar el proyecto en tu m√°quina local, sigue estos pasos:

1. **Construir los contenedores de Docker**:
   ```bash
   docker-compose build

2. **Inicializar la base de datos de Airflow:**
   ```bash
   docker-compose run airflow-init

3. **Iniciar los servicios:**
    ```bash
    docker-compose up -d

# üöÄ Acceder a Airflow

Abre tu navegador y navega a [http://localhost:8080](http://localhost:8080) para acceder a la interfaz de **Airflow** y ejecutar el DAG.

---

## ‚öôÔ∏è Ejecuci√≥n Manual

Si el DAG de Airflow falla, puedes ejecutar manualmente los siguientes notebooks para cada etapa:

- **Ingesta Bronze:** `notebooks/bronze_ingest.ipynb`  
- **Transformaci√≥n Silver:** `notebooks/silver_transform.ipynb`  
- **Agregaci√≥n Gold:** `notebooks/gold_agg.ipynb`

---

## üß† Consultas de Datos

- **Trino:** Accede a las tablas *Silver* y replica los datos en *Gold* usando consultas SQL.  
- **Dremio:** Consulta las tablas *Gold* para realizar an√°lisis.

---

## üåê Puertos y Servicios

| Servicio                          | URL / Puerto                                        | Descripci√≥n                                  |
|-----------------------------------|-----------------------------------------------------|----------------------------------------------|
| **Airflow Webserver**             | [http://localhost:8080](http://localhost:8080)      | Interfaz principal de Airflow                |
| **MinIO (Interfaz)**              | [http://localhost:9000](http://localhost:9000)      | Consola de MinIO                             |
| **MinIO (Console)**               | [http://localhost:9001](http://localhost:9001)      | Administraci√≥n avanzada                      |
| **Jupyter Notebooks**             | [http://localhost:8888](http://localhost:8888)      | Ejecuci√≥n manual de notebooks                |
| **Dremio**                        | [http://localhost:9047](http://localhost:9047)      | Consultas y an√°lisis de datos                |
| **Trino**                         | [http://localhost:8181](http://localhost:8181)      | Motor de consultas SQL distribuidas          |


---

## üîë Acceso a los Servicios

- **Airflow Webserver:** Gestiona y ejecuta tus DAGs ‚Üí [http://localhost:8080](http://localhost:8080)  
- **MinIO:** Interfaz de administraci√≥n ‚Üí [http://localhost:9000](http://localhost:9000)  
- **Jupyter Notebooks:** Ejecuta scripts manualmente ‚Üí [http://localhost:8888](http://localhost:8888)  
- **Dremio:** Analiza datos con SQL ‚Üí [http://localhost:9047](http://localhost:9047)  
- **Trino:** Ejecuta consultas distribuidas ‚Üí [http://localhost:8181](http://localhost:8181)

---

## üß© Servicios del Proyecto

Este proyecto utiliza los siguientes servicios dentro de Docker:

- **MinIO:** Almacenamiento de objetos para datos sin procesar.  
- **Airflow:** Orquestador de tareas del pipeline de datos.  
- **Apache Spark:** Procesamiento de datos en paralelo.  
- **Nessie:** Cat√°logo de metadatos para gesti√≥n de tablas.  
- **Dremio:** Plataforma para consultas SQL.  
- **Trino:** Motor de consultas SQL distribuido.  

---

## üß≠ Diagrama de Arquitectura

<img width="1171" height="1334" alt="image" src="https://github.com/user-attachments/assets/d91a8036-9588-4590-b71b-3fbfd904fe7b" />


