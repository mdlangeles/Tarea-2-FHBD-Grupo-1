# **Proyecto de Arquitectura Lakehouse y Pipeline de Datos**

## **Descripci√≥n**

Este proyecto implementa una arquitectura **Lakehouse** con un pipeline de procesamiento de datos utilizando herramientas y frameworks de big data. El pipeline procesa los datos a trav√©s de tres etapas: **Bronze**, **Silver** y **Gold**, asegurando calidad de los datos, transformaci√≥n y agregaci√≥n.

La arquitectura emplea las siguientes tecnolog√≠as:

- **MinIO** para almacenamiento de objetos
- **Apache Iceberg** para formatos de tablas
- **Apache Spark** (PySpark) para procesamiento de datos
- **Nessie** como cat√°logo de metadatos
- **Apache Airflow** para orquestaci√≥n de pipelines
- **DLT (Data Load Tool)** para ingesta de datos
- **Dremio/Trino** para consultas SQL
- **Jupyter Notebooks** para ejecuci√≥n manual

## **Estructura del Proyecto**

```plaintext
.
‚îú‚îÄ‚îÄ dags/                         # DAGs de Airflow para las tareas del pipeline
‚îú‚îÄ‚îÄ notebooks/                    # Notebooks de Jupyter para ejecuci√≥n manual
‚îú‚îÄ‚îÄ .gitignore                    # Archivo de exclusi√≥n para Git
‚îú‚îÄ‚îÄ Dockerfile.airflow            # Dockerfile para la configuraci√≥n de Airflow
‚îú‚îÄ‚îÄ Dockerfile.jupyter            # Dockerfile para la configuraci√≥n de Jupyter
‚îú‚îÄ‚îÄ docker-compose.yml            # Archivo Docker Compose para desplegar los contenedores
‚îú‚îÄ‚îÄ requirements.txt              # Dependencias de Python necesarias

```

# **Flujo del Proyecto**

### **1. Bronze**

- **Objetivo**: Ingesta de datos sin transformar y almacenamiento en formato Parquet.
- **Ejecuci√≥n**: Se cargan los datos de las tablas **posts** y **users** de 2020 y 2021 utilizando DLT. Las dem√°s tablas deben cargarse manualmente.
- **Tecnolog√≠as**: DLT, MinIO, formato Parquet.

### **2. Silver**

- **Objetivo**: Transformaci√≥n y normalizaci√≥n de los datos hist√≥ricos utilizando el formato Iceberg.
- **Ejecuci√≥n**: Los datos hist√≥ricos de las tablas **posts** y **users** de 2020 y 2021 se procesan y se fusionan.
- **Tecnolog√≠as**: Apache Spark (PySpark), Apache Iceberg.

### **3. Gold**

- **Objetivo**: Agregaci√≥n de datos y generaci√≥n de m√©tricas y KPIs.
- **Ejecuci√≥n**: Se generan m√©tricas como `post_counts_by_user` y `vote_stats_per_post`.
- **Tecnolog√≠as**: Apache Spark (PySpark), Apache Iceberg.

## **C√≥mo Ejecutar el Proyecto**

Para ejecutar el proyecto en tu m√°quina local, sigue estos pasos:

1. **Construir los contenedores de Docker**:
   ```bash
   docker-compose build

2. **Inicializar la base de datos de Airflow:**
   ```bash
   docker-compose run airflow-init

3. **Iniciar los servicios:**
    ```bash
    docker-compose up -d

# Acceder a Minio

Abre tu navegagor y navega a [http://localhost:9001](http://localhost:9001)  para acceder a la interfaz de **Minio** y ver Bucket.

# üöÄ Acceder a Airflow

Abre tu navegador y navega a [http://localhost:8080](http://localhost:8080) para acceder a la interfaz de **Airflow** y ejecutar el DAG.

##  Configuraci√≥n en **Airflow**

### En **Admin ‚Üí Variables**

**Key:** `dlt_secrets_toml`  
**Value:**

```toml
[destination.filesystem]
bucket_url = "s3://lakehouse/bronze"

[destination.filesystem.credentials]
aws_access_key_id = "minioadmin"
aws_secret_access_key = "minioadmin"
endpoint_url = "http://minio:9000"
region_name = "us-east-1"
addressing_style = "path"
```

### En **Admin ‚Üí Connections**

**Connection Id:** `spark_default`  
**Connection Type:** `Spark`  
**Host:** `spark://spark-master`  
**Port:** `7077`

Listo, ahora ejecuta el pipeline en airflow.

---

## ‚öôÔ∏è Ejecuci√≥n Manual

Si el DAG de Airflow falla, puedes ejecutar manualmente los siguientes notebooks para cada etapa:

- **Ingesta Bronze:** `notebooks/bronze_ingest.ipynb`
  Para este notebook, debes tener una carpeta en tu directorio llamada **StackOverflowData**
  Ah√≠ debes tener tus archivos.parquet para la ejecuci√≥n.
  
  <img width="300" height="300" alt="image" src="https://github.com/user-attachments/assets/21cac00d-d333-4f28-a5e9-64467e15acb8" />



  Tambi√©n puedes verificar en Dremio [http://localhost:9047](http://localhost:9047). Donde:

   - Le das en a√±adir **Data Source**.
   - Despu√©s en **Amazon S3**
   - En "name" le das **minio_lakehouse**
   - En AWS Access Key le das "minioadmin"
   - En AWS Access Secret le das "minioadmin" tambi√©n
   - Desactivas la opci√≥n "Encrypt connection"
   - Le das a la opci√≥n "Add bucket":
      1. lakehouse
   - Pasas a "Advanced Options" y activas la opci√≥n "Enable compatibility mode"
   - Ya en **Connection Properties** veras columnas "Name" y "Value".
   - A√±ader√°s 2, y en cada una respectivamente har√°s:
      1. Name = fs.f3a.endpoint / Value = minio:9000
      2. Name = fs.f3a.path.style.access / Value = true
   - Una vez **Guardes** podr√°s consultar las tablas.
  
  
- **Transformaci√≥n Silver:** `notebooks/silver_transform.ipynb`.
  Al ejecutar este notebook, ingresa a minio para verificar que se ejecut√≥ de manera correcta y el bucket llamado "silver" se cre√≥, ah√≠ est√°r√°n nuestros datos.
- **Agregaci√≥n Gold:** `notebooks/gold_agg.ipynb`.
  Ya en este punto, ingresa a trino [http://localhost:8181](http://localhost:8181)

  ### **Explorar el Cat√°logo y los Esquemas**

```sql
SHOW CATALOGS;

SHOW SCHEMAS FROM nessie;

SHOW TABLES FROM nessie.silver;
```

### **Tabla posts**
```sql
SELECT * FROM nessie.silver.posts LIMIT 1;

SELECT * 
FROM nessie.silver.posts 
WHERE owner_user_id = 12487679 
LIMIT 10;
```

### **Tabla users**
```sql
SELECT * FROM nessie.silver.users LIMIT 1;

SELECT * 
FROM nessie.silver.users 
WHERE user_id = 12487679 
LIMIT 1;

```

### **Tabla badges**
```sql
SELECT * FROM nessie.silver.badges LIMIT 1;

```

### **Tabla badges**
```sql
SELECT * FROM nessie.silver.badges LIMIT 1;

```

## **Consultas Anal√≠ticas**
### **N√∫mero de preguntas y respuestas por usuario**
```sql
SELECT 
    p.owner_user_id AS user_id,
    COUNT(CASE WHEN p.post_type_id = 1 THEN 1 END) AS question_count,
    COUNT(CASE WHEN p.post_type_id = 2 THEN 1 END) AS answer_count
FROM nessie.silver.posts AS p
WHERE p.owner_user_id IS NOT NULL
GROUP BY p.owner_user_id
ORDER BY question_count DESC, answer_count DESC
LIMIT 20;

```


### **N√∫mero de preguntas y respuestas con nombres de usuario**
```sql
SELECT 
    u.user_id,
    u.display_name,
    COUNT(CASE WHEN p.post_type_id = 1 THEN 1 END) AS question_count,
    COUNT(CASE WHEN p.post_type_id = 2 THEN 1 END) AS answer_count
FROM nessie.silver.posts AS p
JOIN nessie.silver.users AS u
  ON p.owner_user_id = u.user_id
GROUP BY u.user_id, u.display_name
ORDER BY question_count DESC, answer_count DESC
LIMIT 20;

```

### **Estad√≠sticas de Insignias (Badges)**
```sql
SELECT 
    b.user_id,
    COUNT(*) AS total_badges,
    COUNT(DISTINCT b.badge_name) AS distinct_badge_types,
    SUM(CASE WHEN b.badge_class = 1 THEN 1 ELSE 0 END) AS gold_badges,
    SUM(CASE WHEN b.badge_class = 2 THEN 1 ELSE 0 END) AS silver_badges,
    SUM(CASE WHEN b.badge_class = 3 THEN 1 ELSE 0 END) AS bronze_badges,
    MAX(b.load_date) AS fecha_cargue
FROM nessie.silver.badges AS b
WHERE b.user_id IS NOT NULL
GROUP BY b.user_id
ORDER BY total_badges DESC;


```

### **C√°lculo del Engagement de Usuario**
```sql
SELECT 
    u.user_id,
    u.display_name,
    COALESCE(p.total_posts, 0) AS total_posts,
    COALESCE(c.total_comments, 0) AS total_comments,
    COALESCE(b.total_badges, 0) AS total_badges,
    (COALESCE(p.total_posts, 0) +
     COALESCE(c.total_comments, 0) +
     COALESCE(b.total_badges, 0)) AS engagement_score,
    MAX(u.load_date) AS fecha_cargue
FROM nessie.silver.users AS u
LEFT JOIN (
    SELECT owner_user_id AS user_id, COUNT(*) AS total_posts
    FROM nessie.silver.posts
    WHERE owner_user_id IS NOT NULL
    GROUP BY owner_user_id
) AS p ON u.user_id = p.user_id
LEFT JOIN (
    SELECT user_id, COUNT(*) AS total_comments
    FROM nessie.silver.comments
    WHERE user_id IS NOT NULL
    GROUP BY user_id
) AS c ON u.user_id = c.user_id
LEFT JOIN (
    SELECT user_id, COUNT(*) AS total_badges
    FROM nessie.silver.badges
    WHERE user_id IS NOT NULL
    GROUP BY user_id
) AS b ON u.user_id = b.user_id
WHERE u.user_id IS NOT NULL
GROUP BY 
    u.user_id, 
    u.display_name, 
    p.total_posts, 
    c.total_comments, 
    b.total_badges
ORDER BY engagement_score DESC;

```

---

## üß† Consultas de Datos

- **Trino:** Accede a las tablas *Silver* y replica los datos en *Gold* usando consultas SQL.  
- **Dremio:** Consulta las tablas *Gold* para realizar an√°lisis.

---

## üåê Puertos y Servicios

| Servicio                          | URL / Puerto                                        | Descripci√≥n                                  |
|-----------------------------------|-----------------------------------------------------|----------------------------------------------|
| **Airflow Webserver**             | [http://localhost:8080](http://localhost:8080)      | Interfaz principal de Airflow                |
| **MinIO (Console)**               | [http://localhost:9001](http://localhost:9001)      | Administraci√≥n avanzada                      |
| **Jupyter Notebooks**             | [http://localhost:8888](http://localhost:8888)      | Ejecuci√≥n manual de notebooks                |
| **Dremio**                        | [http://localhost:9047](http://localhost:9047)      | Consultas y an√°lisis de datos                |
| **Trino**                         | [http://localhost:8181](http://localhost:8181)      | Motor de consultas SQL distribuidas          |


---

## üîë Acceso a los Servicios

- **Airflow Webserver:** Gestiona y ejecuta tus DAGs ‚Üí [http://localhost:8080](http://localhost:8080)  
- **MinIO:** Interfaz de administraci√≥n ‚Üí [http://localhost:9000](http://localhost:9000)  
- **Jupyter Notebooks:** Ejecuta scripts manualmente ‚Üí [http://localhost:8888](http://localhost:8888)  
- **Dremio:** Analiza datos con SQL ‚Üí [http://localhost:9047](http://localhost:9047)  
- **Trino:** Ejecuta consultas distribuidas ‚Üí [http://localhost:8181](http://localhost:8181)

---

## üß© Servicios del Proyecto

Este proyecto utiliza los siguientes servicios dentro de Docker:

- **MinIO:** Almacenamiento de objetos para datos sin procesar.  
- **Airflow:** Orquestador de tareas del pipeline de datos.  
- **Apache Spark:** Procesamiento de datos en paralelo.  
- **Nessie:** Cat√°logo de metadatos para gesti√≥n de tablas.  
- **Dremio:** Plataforma para consultas SQL.  
- **Trino:** Motor de consultas SQL distribuido.  

---

## üß≠ Diagrama de Arquitectura

<img width="1171" height="1334" alt="image" src="https://github.com/user-attachments/assets/d91a8036-9588-4590-b71b-3fbfd904fe7b" />
